{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT and ViT Model Deployment with FastAPI\n",
    "\n",
    "This notebook implements and tests FastAPI endpoints for GPT-2 and ViT models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "!pip install fastapi uvicorn transformers torch python-multipart pytest httpx python-dotenv numpy Pillow pydantic nest-asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, ViTModel, ViTImageProcessor\n",
    "import torch\n",
    "from PIL import Image\n",
    "import io\n",
    "import base64\n",
    "from typing import Optional\n",
    "import time\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "from fastapi.testclient import TestClient\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Enable nested async loops (required for Colab)\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create FastAPI application\n",
    "app = FastAPI()\n",
    "\n",
    "# Load models\n",
    "print(\"Loading models...\")\n",
    "try:\n",
    "    # GPT-2 model\n",
    "    gpt_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    \n",
    "    # ViT model\n",
    "    vit_model = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "    vit_processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "    \n",
    "    # Move models to GPU if available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    gpt_model.to(device)\n",
    "    vit_model.to(device)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading models: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define request models\n",
    "class TextRequest(BaseModel):\n",
    "    text: str\n",
    "    max_length: Optional[int] = 100\n",
    "\n",
    "class ImageRequest(BaseModel):\n",
    "    image: str  # base64 encoded image\n",
    "\n",
    "# Define API endpoints\n",
    "@app.post(\"/generate_text\")\n",
    "async def generate_text(request: TextRequest):\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        inputs = gpt_tokenizer(request.text, return_tensors=\"pt\").to(device)\n",
    "        outputs = gpt_model.generate(\n",
    "            **inputs,\n",
    "            max_length=request.max_length,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=gpt_tokenizer.eos_token_id\n",
    "        )\n",
    "        generated_text = gpt_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        process_time = time.time() - start_time\n",
    "        return {\n",
    "            \"generated_text\": generated_text,\n",
    "            \"processing_time\": process_time\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/process_image\")\n",
    "async def process_image(request: ImageRequest):\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        # Decode base64 image\n",
    "        image_bytes = base64.b64decode(request.image)\n",
    "        image = Image.open(io.BytesIO(image_bytes))\n",
    "        \n",
    "        # Process image\n",
    "        inputs = vit_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "        outputs = vit_model(**inputs)\n",
    "        \n",
    "        # Get the [CLS] token representation\n",
    "        cls_token = outputs.last_hidden_state[:, 0].cpu().detach().numpy()\n",
    "        \n",
    "        process_time = time.time() - start_time\n",
    "        return {\n",
    "            \"embedding_shape\": cls_token.shape,\n",
    "            \"processing_time\": process_time\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    return {\"status\": \"healthy\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Start the server in a separate thread\n",
    "import threading\n",
    "def run_server():\n",
    "    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n",
    "\n",
    "server_thread = threading.Thread(target=run_server)\n",
    "server_thread.daemon = True\n",
    "server_thread.start()\n",
    "print(\"Server started at http://127.0.0.1:8000\")\n",
    "\n",
    "# Wait a bit for the server to start\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Testing functions\n",
    "def create_test_image():\n",
    "    # Create a simple test image\n",
    "    img = Image.new('RGB', (224, 224), color='red')\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    img.save(img_byte_arr, format='PNG')\n",
    "    img_byte_arr = img_byte_arr.getvalue()\n",
    "    return base64.b64encode(img_byte_arr).decode()\n",
    "\n",
    "client = TestClient(app)\n",
    "\n",
    "def test_health():\n",
    "    response = client.get(\"/health\")\n",
    "    assert response.status_code == 200\n",
    "    assert response.json() == {\"status\": \"healthy\"}\n",
    "    print(\"Health check passed!\")\n",
    "\n",
    "def run_performance_tests():\n",
    "    # Test health endpoint\n",
    "    test_health()\n",
    "    \n",
    "    # Test GPT endpoint\n",
    "    print(\"\\nTesting GPT API Performance...\")\n",
    "    gpt_results = []\n",
    "    test_text = \"Once upon a time\"\n",
    "    \n",
    "    for i in range(100):\n",
    "        start_time = time.time()\n",
    "        response = client.post(\n",
    "            \"/generate_text\",\n",
    "            json={\"text\": test_text, \"max_length\": 50}\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        \n",
    "        assert response.status_code == 200\n",
    "        \n",
    "        result = {\n",
    "            \"request_number\": i + 1,\n",
    "            \"total_time\": end_time - start_time,\n",
    "            \"server_processing_time\": response.json()[\"processing_time\"]\n",
    "        }\n",
    "        gpt_results.append(result)\n",
    "        print(f\"GPT Request {i+1}/100 completed in {result['total_time']:.3f} seconds\")\n",
    "    \n",
    "    # Test ViT endpoint\n",
    "    print(\"\\nTesting ViT API Performance...\")\n",
    "    vit_results = []\n",
    "    test_image = create_test_image()\n",
    "    \n",
    "    for i in range(100):\n",
    "        start_time = time.time()\n",
    "        response = client.post(\n",
    "            \"/process_image\",\n",
    "            json={\"image\": test_image}\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        \n",
    "        assert response.status_code == 200\n",
    "        \n",
    "        result = {\n",
    "            \"request_number\": i + 1,\n",
    "            \"total_time\": end_time - start_time,\n",
    "            \"server_processing_time\": response.json()[\"processing_time\"]\n",
    "        }\n",
    "        vit_results.append(result)\n",
    "        print(f\"ViT Request {i+1}/100 completed in {result['total_time']:.3f} seconds\")\n",
    "    \n",
    "    # Calculate and display statistics\n",
    "    def calculate_stats(results, model_name):\n",
    "        total_times = [r[\"total_time\"] for r in results]\n",
    "        processing_times = [r[\"server_processing_time\"] for r in results]\n",
    "        \n",
    "        stats = {\n",
    "            \"average_total_time\": np.mean(total_times),\n",
    "            \"average_processing_time\": np.mean(processing_times),\n",
    "            \"min_time\": np.min(total_times),\n",
    "            \"max_time\": np.max(total_times),\n",
    "            \"std_dev\": np.std(total_times)\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{model_name} Performance Statistics:\")\n",
    "        print(f\"Average Total Time: {stats['average_total_time']:.3f} seconds\")\n",
    "        print(f\"Average Processing Time: {stats['average_processing_time']:.3f} seconds\")\n",
    "        print(f\"Min Time: {stats['min_time']:.3f} seconds\")\n",
    "        print(f\"Max Time: {stats['max_time']:.3f} seconds\")\n",
    "        print(f\"Standard Deviation: {stats['std_dev']:.3f} seconds\")\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    gpt_stats = calculate_stats(gpt_results, \"GPT\")\n",
    "    vit_stats = calculate_stats(vit_results, \"ViT\")\n",
    "    \n",
    "    # Save results\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results = {\n",
    "        \"gpt\": {\n",
    "            \"individual_results\": gpt_results,\n",
    "            \"statistics\": gpt_stats\n",
    "        },\n",
    "        \"vit\": {\n",
    "            \"individual_results\": vit_results,\n",
    "            \"statistics\": vit_stats\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(f\"test_results_{timestamp}.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    print(f\"\\nDetailed results saved to test_results_{timestamp}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run the performance tests\n",
    "run_performance_tests()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
